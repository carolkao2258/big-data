# -*- coding: utf-8 -*-
"""
Created on Mon May 24 10:08:27 2021

@author: Shawn
"""

import pandas as pd

import os
os.chdir("C:\Users\uer\Downloads\\Sec 02 Regular Expression and Coupus Cleaning")

file = "reddit_dataframe.pkl"
df = pd.read_pickle(file)

YahooNews=pd.read_csv('ç ”ç©¶å ±å‘Šâ€”å€‹è‚¡09-12_utf8.csv')

text = """
After viewing the [PINKIEPOOL Trailer](https://www.youtu.be/watch?v=ieHRoHUg)
it got me thinking about the best match ups.
<lb>Here's my take:<lb><lb>[](/sp)[](/ppseesyou) Deadpool<lb>[](/sp)[](/ajsly)
Captain America<lb>"""

text1 = "èšç„¦åœ‹å…§ã€æŠ—è¡°é€€ã€æœ‰æ´¾æ¯ï¼åˆ†æå¸«ï¼šåº·å¡æ–¯ç‰¹é‚„èƒ½å†æ¼²20%,\
é‰…äº¨ç¶² ,20190906,13:40,åº·å¡æ–¯ç‰¹ (Comcast) (CMCSA-US) é€±å››ç²åˆ¸å•†èª¿é«˜è©•ç´šè‡³ç­‰åŒè²·é€²ï¼Œè‚¡åƒ¹å†æ·»åŠ©åŠ›ã€‚ Oppenheimer åˆ†æå¸« Timothy Horan èªç‚ºï¼Œå¯¬é »ç‡Ÿæ”¶æˆé•·ã€åˆ©æ½¤æ“´å¤§å’Œç›¸å°ä½çš„ä¼°å€¼éƒ½æ˜¯è²·é€² Comcast è‚¡ç¥¨çš„ç†ç”±ï¼Œå„˜ç®¡ç›®å‰è©²è‚¡ä»Šå¹´å·²ä¸Šæ¼²ç´„ 35%(å«è‚¡åˆ©)ï¼Œ\
<em>Horan ä¼°è¨ˆè©²è‚¡ä»é­ä½ä¼°è¿‘ 20%ã€‚</em> <h6>èƒŒæ™¯</h6> éš¨è‘—å¤šæ•¸åª’é«”å’Œé›»ä¿¡ç”¢æ¥­ç™¼å±•ï¼ŒComcast å»å¹´ä¹ŸåŠ å…¥ä½µè³¼å¤§æˆ°ï¼Œå»å¹´èˆ‡è¿ªå£«å°¼ (DIS) ç«¶è³¼ 21 ä¸–ç´€ç¦æ–¯çš„å¨›æ¨‚è³‡ç”¢ï¼Œæ¥è‘—å»å¹´ç§‹å­£åˆåƒèˆ‡æ­æ´²ä»˜è²»é›»è¦–ç‡Ÿé‹å•† Sky çš„æ‹è³£ï¼Œé›–ç„¶ä¸æ•µè¿ªå£«å°¼ï¼Œæœªèƒ½è²·ä¸‹ç¦æ–¯è³‡ç”¢ï¼Œ\
ä½†å»æˆåŠŸä»¥äº¤æ˜“åƒ¹æ ¼ 461 å„„ç¾å…ƒ (å«å‚µå‹™) è´å¾— Sky çš„æ§åˆ¶æ¬Šï¼Œæ”¶è³¼å…¶ä½æ–¼æ­æ´² 7 å€‹åœ‹å®¶ 2300 è¬å€‹è¡›æ˜Ÿé›»è¦–ç”¨æˆ¶ï¼Œä»¥åŠå…¶ Now TV ä¸²æµå¹³å°ç´„ 200 è¬åè¨‚é–±æˆ¶ã€‚ æŠ•è³‡è€…æœ€åˆä¸¦æœªçœ‹å¥½é€™äº›äº¤æ˜“ï¼Œå› ç‚ºä»–å€‘æ“”å¿ƒ Comcast è²·è²´äº†ï¼Œä¸”è¡›æ˜Ÿé›»è¦–è¶¨å‹¢å‘ˆè² é¢ï¼Œ\
å› æ­¤ Comcast å»å¹´è‚¡åƒ¹ä»¥ä¸‹è·Œ 15% æ”¶å ´ï¼Œç›´è‡³ä»Šå¹´æ‰å›å½ˆã€‚Comcast ä»Šå¹´å› æ ¸å¿ƒæœ‰ç·šé›»è¦–æ¥­å‹™åœ¨ Xfinity å“ç‰Œçš„æ”¯æ’ä¸‹ï¼Œæˆç¸¾æ–ç„¶ï¼Œå› æ­¤ç©©å®šæ¨å‡è‚¡åƒ¹ã€‚ æŠ•è³‡è€…å°ç¶“æ¿Ÿæˆé•·è¶¨ç·©ã€è²¿æ˜“æˆ°ã€ä»Šå¹´å‚µåˆ¸æ®–åˆ©ç‡å¤§è·Œçš„æ“”æ†‚å¿ƒè£¡å‡¸é¡¯äº†æœ‰ç·šé›»è¦–ã€Œèšç„¦åœ‹å…§ã€ã€ã€ŒæŠ—ç¶“æ¿Ÿè¡°é€€ã€çš„å•†æ¥­æ¨¡å¼ï¼Œ\
ä¸”æ´¾æ¯è‚¡åœ¨æ­¤æ™‚åˆæ›´åŠ è¿·äººï¼Œå› æ­¤ä»Šå¹´è¿„ä»Š Comcast è‚¡åƒ¹ç´„ä¸Šæ¼² 35%ï¼Œå„ªæ–¼åŒæœŸ S&amp;P 500 æŒ‡æ•¸ 18.8% çš„æ”¶ç›Šè¡¨ç¾ï¼Œå¦å¤–åŒæ¥­ Charter Communications (CHTR-US) ä¸Šæ¼² 47.2%ã€Altice USA (ATUS-US) ä¸Šæ¼² 75.5% ç­‰ã€‚ \
<figure><img src=""https://s.yimg.com/uu/api/res/1.2/GgJDsAmVP.vyCqSfK5rEGg--/YXBwaWQ9eXRhY2h5b247cT03NTs-/https://media.zenfs.com/zh/cnyes.com/59458dcf186ed236719d9446e6762cf3"">\
</figure><h6>æ’éå‰ªç·šæ½®ï¼Ÿ</h6> éš¨è‘—æœ‰ç·šé›»è¦–è¨‚é–±äººæ•¸ä¸æ–·ä¸‹é™ï¼Œç¶²è·¯èˆ‡æœ‰ç·šé›»è¦–æ¥­å‹™ä¸å†ç¶åœ¨ä¸€èµ·å‡ºå”®ï¼Œä»Šå¹´æœ‰ç·šé›»è¦–å…¬å¸ä¸æ–·ç™¼å±•å¯¬é »ç¶²è·¯äº‹æ¥­ï¼ŒæŠ•è³‡è€…ä¹Ÿæœ‰äº†æ–°çš„èªè­˜ã€‚ Oppenheimer åˆ†æå¸« Horan èªç‚ºï¼Œå…¶å¯¬é »åŠ›é“æŒçºŒä¸¦æ”¯æ’åˆ©æ½¤ä¸Šå‡ï¼Œé™ä½è³‡æœ¬æ”¯å‡ºçš„è¦æ±‚ã€‚ \
ã€Œæ‹†åˆ†ç¶“æ¿Ÿæœ‰åŠ©æ“´å¤§åˆ©æ½¤ï¼Œå› ç‚ºé¡§å®¢æ›´å‚¾å‘å–®ç¨è³¼è²· Comcast çš„å¯¬é »æœå‹™ã€‚ç”±æ–¼ç‡Ÿæ”¶æ··åˆæ–¹å¼è½‰å‘åˆ©æ½¤æ›´é«˜çš„å¯¬é » (ä¼°è¨ˆ 70% EBITDA åˆ©æ½¤)ï¼Œè€Œéå½±è¦– (ä¼°è¨ˆ 20% åˆ©æ½¤)ï¼Œæœ‰ç·šé›»çºœçš„åˆ©æ½¤æ‡‰è©²æœƒæŒçºŒæ“´å¤§ã€‚ã€\
<figure><img src=""https://s1.yimg.com/uu/api/res/1.2/oo2yUJD0PVnhKFw.QeWs.w--/YXBwaWQ9eXRhY2h5b247cT03NTs-/https://media.zenfs.com/zh/cnyes.com/f67abbf18565746acab385220d1a4b14""></figure>Horan çœ‹åˆ°ï¼Œ\
Comcast æŒçºŒå°‡ç¾è¡Œç¶²è·¯è¨‚é–±æˆ¶æ¨å‘æ•¸æ“šå‚³è¼¸æ›´å¿«çš„æœå‹™ï¼Œå› å…¶åƒ¹ä½æ›´é«˜ã€‚åŒæ¥­ Charter è¨ˆç•«æ˜å¹´èµ·èª¿æ¼²åƒ¹æ ¼ï¼ŒHoran é æœŸ Comcast ä¹Ÿå°‡è·Ÿé€²ï¼Œå› æ­¤å¯æœ›èª¿å‡æ¯æœˆç”¨æˆ¶çš„å¹³å‡ä¸€å¹´ç‡Ÿæ”¶ 4%ã€‚ <h6>å‰æ™¯</h6> Horan ä¼°è¨ˆ Comcast ä»Šå¹´ Ebitda ä¸Šå‡ 2 å€‹ç™¾åˆ†é»ï¼Œ\
æ¥è‘— 2020 å¹´å†ä¸Šå‡ 0.7 å€‹ç™¾åˆ†é»é” 32.2%ã€‚èˆ‡æ­¤åŒæ™‚ï¼Œä»–ä¼°è¨ˆè©²å…¬å¸è³‡æœ¬æ”¯å‡ºç©©å®šï¼Œä¸”ä»Šå¹´ç‡Ÿæ”¶æˆé•· 3.5%ï¼Œä¾†å¹´æˆé•· 4.5%ã€‚ å…¶çµæœæ˜¯ Comcast 2020 å¹´è‡ªç”±ç¾é‡‘æµä¸Šå‡ 15%ï¼Œè‡³ä¼°è¨ˆè¿‘ 190 å„„ç¾å…ƒï¼Œè©²å…¬å¸å±†æ™‚å¯èƒ½ç”¨æ–¼æ´¾æ¯ã€åŸ·è¡Œåº«è—è‚¡æˆ–è€…æ¸…å„Ÿå‚µå‹™ã€‚\
?Horan å°è©²å…¬å¸ç›®æ¨™åƒ¹ 54 ç¾å…ƒï¼Œæ˜¯åŸºæ–¼ 2020 å¹´ç¾é‡‘æµæ”¶ç›Š 7.5% ä¾†ä¼°è¨ˆã€‚"

import regex as re1

def tokenize(text):
    return re1.findall(r'[\w-]*\p{L}[\w-]*',text)

tokens = tokenize(text)
tokens[0]

from collections import Counter

tokens = tokenize('She likes my cats and my cats like my sofa')

counter = Counter(tokens)

more_tokens=tokenize('She likes dogs and cats')
counter.update(more_tokens)
counter

df['tokens']=df['text'].apply(tokenize)
df['tokens'][10]

counter = Counter()
df['tokens'].map(counter.update)

print(counter.most_common(5))

min_freq = 2
freq_df = pd.DataFrame.from_dict\
    (counter,orient='index',columns=['freq'])

freq_df.index.name = 'token'
freq_df = freq_df.sort_values('freq',ascending=False)
freq_df.head(10)

ax=freq_df.head(15).plot(kind='barh')
ax.invert_yaxis()

from matplotlib import pyplot as plt
from wordcloud import WordCloud

wordcloud = WordCloud(background_color='white')
wordcloud.generate_from_frequencies(freq_df['freq'])
plt.imshow(wordcloud)

#conda install cctbx202211::pillow

import re

RE_SUSPICIOUS = re.compile(r'[&#<>{}\[\]\\]')

def impurity(text, min_len=10):
    """returns the share of suspicious characters in a text"""
    if text == None or len(text) < min_len:
        return 0
    else:
        return len(RE_SUSPICIOUS.findall(text))/len(text)

print(impurity(text))

print(impurity(text1))

df['impurity'] = df['text'].apply(impurity, min_len=10)
df.columns
df['impurity'][:2]
df[['text', 'impurity']].sort_values(by='impurity', ascending=False).head(3)

YahooNews['Impurity']=YahooNews['Context'].apply(impurity, min_len=10)
YahooNews.columns
YahooNews[['Context', 'Impurity']].sort_values(by='Impurity', ascending=False).head(3)

import html
p = '&lt;abc&gt;' #&lt; and &gt; are special simbles in html
#not showing in text example
txt= html.unescape(p)
print (txt)

import html

def clean(text):
    # convert html escapes like &amp; to characters.
    text = html.unescape(text) #in this example, this part does nothing
    # tags like <tab>
    text = re.sub(r'<[^<>]*>', ' ', text)
    # markdown URLs like [Some text](https://....)
    text = re.sub(r'\[([^\[\]]*)\]\([^\(\)]*\)', r'\1', text)
    # text or code in brackets like [0]
    text = re.sub(r'\[[^\[\]]*\]', ' ', text)
    # standalone sequences of specials, matches &# but not #cool
    text = re.sub(r'(?:^|\s)[&#<>{}\[\]+|\\:-]{1,}(?:\s|$)', ' ', text)
    # standalone sequences of hyphens like --- or ==
    text = re.sub(r'(?:^|\s)[\-=\+]{2,}(?:\s|$)', ' ', text)
    # sequences of white spaces
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

clean_text = clean(text)
print(clean_text)
print("Impurity:", impurity(clean_text))

clean_text1 = clean(text1)
print(clean_text1)
print("Impurity:", impurity(clean_text1))


df['clean_text'] = df['text'].map(clean)
df['impurity']   = df['clean_text'].apply(impurity, min_len=20)

df[['clean_text', 'impurity']].sort_values(by='impurity', ascending=False) \
                              .head(3)
                              
                              
YahooNews['Clean_text'] = YahooNews['Context'].map(clean)
YahooNews['Impurity']   = YahooNews['Clean_text'].apply(impurity, min_len=20)

YahooNews[['Clean_text', 'Impurity']].sort_values(by='Impurity', ascending=False) \
                              .head(3) 


text = "The cafÃ© â€œSaint-RaphaÃ«lâ€ is loca-\nted on CÃ´te dÊ¼Azur."
#print(textacy.__version__)
import textacy.preprocessing as tprep
#you need to install textacy
def normalize(text):
    text = tprep.normalize.hyphenated_words(text)
    text = tprep.normalize.quotation_marks(text)
    text = tprep.normalize.unicode(text)
    text = tprep.remove.accents(text)
    return text

print(normalize(text))


from textacy.preprocessing import replace

text = "Check out https://spacy.io/usage/spacy-101"

# using default substitution _URL_
print(replace.urls(text))
print(replace.urls(text1))

#for df:

df['clean_text'] = df['clean_text'].map(replace.urls)
df['clean_text'] = df['clean_text'].map(normalize)

YahooNews['Clean_text']=YahooNews['Clean_text'].map(replace.urls)

df.rename(columns={'text': 'raw_text', 'clean_text': 'text'}, inplace=True)
df.drop(columns=['impurity'], inplace=True)

YahooNews.rename(columns={'Context': 'Raw_text', 'Clean_text': 'Context'}, inplace=True)
YahooNews.drop(columns=['Impurity'], inplace=True)






###Linguistic Processing

##Because this part is very complicated, I will demo the whole process first.
##Then I will explain each part later

###English###
dfs=df.iloc[:200,:]

import spacy
nlp = spacy.load('en_core_web_sm')

dfs['doc']=dfs['text'].apply(nlp)

import textacy

def extract_lemmas(doc, **kwargs):
    return [t.lemma_ for t in textacy.extract.basics.words(doc, **kwargs)]


dfs['lemmas'] = dfs['doc'].apply(extract_lemmas, include_pos=['ADJ', 'NOUN'])
dfs['lemmas']

from collections import Counter
counter = Counter()#use a empty string first
dfs['lemmas'].map(counter.update)

print(counter.most_common(5))
# transform counter into data frame
min_freq=2
#transform dict into dataframe
freq_df = pd.DataFrame.from_dict(counter, orient='index', columns=['freq'])
freq_df = freq_df.query('freq >= @min_freq')
freq_df.index.name = 'token'
freq_df = freq_df.sort_values('freq', ascending=False)
freq_df.head(15)


ax = freq_df.head(15).plot(kind='barh', width=0.95, figsize=(8,3))
ax.invert_yaxis()
ax.set(xlabel='Frequency', ylabel='Token', title='Top Words')

###Creating Word Clouds
from matplotlib import pyplot as plt
from wordcloud import WordCloud ###
from collections import Counter ###

wordcloud = WordCloud(background_color='white')
wordcloud.generate_from_frequencies(freq_df['freq'])
plt.imshow(wordcloud)


###Chinese###
#1åŠ å…¥ç¹é«”è©å…¸
import jieba

jieba.set_dictionary('./çµå·´/dict.txt.big.txt')
stopwords1 = [line.strip() for line in open('./çµå·´/stopWords.txt', 'r', encoding='utf-8').readlines()]

def remove_stop(text):
    c1=[]
    for w in text:
        if w not in stopwords1:
            c1.append(w)
    c2=[i for i in c1 if i.strip() != '']
    return c2



YahooNews['tokens']=YahooNews['Context'].apply(jieba.cut)
YahooNews['tokens_new']=YahooNews['tokens'].apply(remove_stop)
YahooNews.iloc[0,:]


#Freq charts
from collections import Counter
counter = Counter()#use a empty string first
YahooNews['tokens_new'].apply(counter.update)
print(counter.most_common(15))

import seaborn as sns
sns.set(font="SimSun")
min_freq=2
#transform dict into dataframe
freq_df = pd.DataFrame.from_dict(counter, orient='index', columns=['freq'])
freq_df = freq_df.query('freq >= @min_freq')
freq_df.index.name = 'token'
freq_df = freq_df.sort_values('freq', ascending=False)
freq_df.head(15)

ax = freq_df.head(15).plot(kind='barh', width=0.95, figsize=(8,3))
ax.invert_yaxis()
ax.set(xlabel='Frequency', ylabel='Token', title='Top Words')

###Creating Word Clouds
from matplotlib import pyplot as plt
from wordcloud import WordCloud ###
from collections import Counter ###

wordcloud = WordCloud(font_path="./çµå·´/SimHei.ttf", background_color="white")
wordcloud.generate_from_frequencies(freq_df['freq'])
#plt.figure(figsize=(20,10)) 
plt.imshow(wordcloud)




###Some Details

text = "The cafÃ© â€œSaint-RaphaÃ«lâ€ is loca-\nted on CÃ´te dÊ¼Azur."
#print(textacy.__version__)
import textacy.preprocessing as tprep
#you need to install textacy
def normalize(text):
    text = tprep.normalize.hyphenated_words(text)
    text = tprep.normalize.quotation_marks(text)
    text = tprep.normalize.unicode(text)
    text = tprep.remove.accents(text)
    return text

print(normalize(text))


from textacy.preprocessing import replace

text = "Check out https://spacy.io/usage/spacy-101"

# using default substitution _URL_
print(replace.urls(text))
print(replace.urls(text1))

#for df:

df['clean_text'] = df['clean_text'].map(replace.urls)
df['clean_text'] = df['clean_text'].map(normalize)

YahooNews['Clean_text']=YahooNews['Clean_text'].map(replace.urls)

df.rename(columns={'text': 'raw_text', 'clean_text': 'text'}, inplace=True)
df.drop(columns=['impurity'], inplace=True)

YahooNews.rename(columns={'Context': 'Raw_text', 'Clean_text': 'Context'}, inplace=True)
YahooNews.drop(columns=['Impurity'], inplace=True)

text = """
2019-08-10 23:32: @pete/@louis - I don't have a well-designed 
solution for today's problem. The code of module AC68 should be -1. 
Have to think a bit... #goodnight ;-) ğŸ˜©ğŸ˜¬"""



tokens = re.findall(r'\w\w+', text)
print(*tokens, sep='|')

RE_TOKEN = re.compile(r"""
               ( [#]?[@\w'â€™\.\-\:]*\w     # words, hash tags and email adresses
               | [:;<]\-?[\)\(3]          # coarse pattern for basic text emojis
               | [\U0001F100-\U0001FFFF]  # coarse code range for unicode emojis
               )
               """, re.VERBOSE)

def tokenize(text):
    return RE_TOKEN.findall(text)

tokens = tokenize(text)
print(*tokens, sep='|')



import nltk

nltk.download('punkt') ###
tokens = nltk.tokenize.word_tokenize(text)
print(*tokens, sep='|')


tokenizer = nltk.tokenize.RegexpTokenizer(RE_TOKEN.pattern, flags=re.VERBOSE)
tokens = tokenizer.tokenize(text)
print(*tokens, sep='|')


tokenizer = nltk.tokenize.TweetTokenizer()
tokens = tokenizer.tokenize(text)
print(*tokens, sep='|')


tokenizer = nltk.tokenize.ToktokTokenizer()
tokens = tokenizer.tokenize(text)
print(*tokens, sep='|')


import spacy
nlp = spacy.load('en_core_web_sm')

nlp.pipeline
nlp = spacy.load("en_core_web_sm", disable=["parser", "ner"])

nlp = spacy.load("en_core_web_sm")
text = "My best friend Ryan Peters likes fancy adventure games."
doc = nlp(text)

for token in doc:
    print(token, end="|")
    
    
def display_nlp(doc, include_punct=False):
    """Generate data frame for visualization of spaCy tokens."""
    rows = []
    for i, t in enumerate(doc):
        if not t.is_punct or include_punct:
            row = {'token': i,  'text': t.text, 'lemma_': t.lemma_, 
                   'is_stop': t.is_stop, 'is_alpha': t.is_alpha,
                   'pos_': t.pos_, 'dep_': t.dep_, 
                   'ent_type_': t.ent_type_, 'ent_iob_': t.ent_iob_}
            rows.append(row)
    
    df = pd.DataFrame(rows).set_index('token')
    df.index.name = None
    return df    
    
display_nlp(doc)  
    
  
    
    
   
nlp = spacy.load('en_core_web_sm') ###
text = "Dear Ryan, we need to sit down and talk. Regards, Pete"
doc = nlp(text)

non_stop = [t for t in doc if not t.is_stop and not t.is_punct]
print(non_stop)    
    
nlp = spacy.load('en_core_web_sm')
nlp.vocab['down'].is_stop = False
nlp.vocab['Dear'].is_stop = True
nlp.vocab['Regards'].is_stop = True


nlp = spacy.load('en_core_web_sm')



text = "My best friend Ryan Peters likes fancy adventure games."
doc = nlp(text)

print(*[t.lemma_ for t in doc], sep='|')

text = "My best friend Ryan Peters likes fancy adventure games."
doc = nlp(text)

nouns = [t for t in doc if t.pos_ in ['NOUN', 'PROPN']]
print(nouns)

import textacy

tokens = textacy.extract.words(doc, 
            filter_stops = True,           # default True, no stopwords
            filter_punct = True,           # default True, no punctuation
            filter_nums = True,            # default False, no numbers
            include_pos = ['ADJ', 'NOUN'], # default None = include all
            exclude_pos = None,            # default None = exclude none
            min_freq = 1)                  # minimum frequency of words

print(*[t for t in tokens], sep='|')


def extract_lemmas(doc, **kwargs):
    return [t.lemma_ for t in textacy.extract.words(doc, **kwargs)]

lemmas = extract_lemmas(doc, include_pos=['ADJ', 'NOUN'])
print(*lemmas, sep='|')



text = "My best friend Ryan Peters likes fancy adventure games."
doc = nlp(text)

patterns = ["POS:ADJ POS:NOUN:+"]
spans = textacy.extract.matches.token_matches(doc, patterns=patterns)
print(*[s.lemma_ for s in spans], sep='|')


print(*doc.noun_chunks, sep='|')

def extract_noun_phrases(doc, preceding_pos=['NOUN'], sep='_'):
    patterns = []
    for pos in preceding_pos:
        patterns.append(f"POS:{pos} POS:NOUN:+")
    spans = textacy.extract.matches.token_matches(doc, patterns=patterns)
    return [sep.join([t.lemma_ for t in s]) for s in spans]

print(*extract_noun_phrases(doc, ['ADJ', 'NOUN']), sep='|')




text = "James O'Neill, chairman of World Cargo Inc, lives in San Francisco."
doc = nlp(text)

for ent in doc.ents:
    print(f"({ent.text}, {ent.label_})", end=" ")


from spacy import displacy

displacy.serve(doc, style='ent')
displacy.serve(doc, style='dep')

def extract_entities(doc, include_types=None, sep='_'):

    ents = textacy.extract.entities(doc, 
             include_types=include_types, 
             exclude_types=None, 
             drop_determiners=True, 
             min_freq=1)
    
    return [sep.join([t.lemma_ for t in e])+'/'+e.label_ for e in ents]

print(extract_entities(doc, ['PERSON', 'GPE']))




